# -*- coding: utf-8 -*-
"""ML Assignment 1_Umesh.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E1RT4XXLXbCdLUDOFtUJd72QCmzZ0ZBk

# **Q1: Data Preprocessing**

You have been provided with a CSV file "Cars93.csv." The given dataset is related to cars and contains 26 columns. In the given dataset, “Price” is the target variable (i.e., the output). The marks distribution according to the tasks are as follows:
1. Assign a type to each of the following features (a) Model, (b) Type, (c) Max. Price and (d) Airbags from the following: ordinal/nominal/ratio/interval scale.
2. Write a function to handle the missing values in the dataset (e.g., any NA, NaN values).
3. Write a function to reduce noise (any error in the feature) in individual attributes.
4. Write a function to encode all the categorical features in the dataset according to the type of variable jointly.
5. Write a function to normalize / scale the features either individually or jointly.
6. Write a function to create a random split of the data into train, validation and test sets in the ratio of [70:20:10].
"""

from pathlib import Path
import pandas as pd
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
def load_car_data():
    return pd.read_csv(Path("/content/sample_data/Cars93.csv"))

df = load_car_data()

"""**Features Classifications**"""

def classify_feature(feature_name):
    if df[feature_name].dtype == 'object':  # Categorical features
        if feature_name in ["Model", "Type"]:
            return "Nominal"
        elif feature_name == "AirBags":
            return "Ordinal"
    elif df[feature_name].dtype in ['int64', 'float64']:  # Numerical features
        if feature_name == "Max.Price":
            return "Ratio"
    return "Unknown"

# List of features to classify
features = ["Model", "Type", "Max.Price", "AirBags"]

# Apply the classification function
feature_types = {feature: classify_feature(feature) for feature in features}

# Print the results
for feature, feature_type in feature_types.items():
    print(f"{feature}: {feature_type}")

"""**Handling Missing Values**"""

def handle_missing_values(df):
    """Fills missing values with median for numerical columns."""
    for column in df.columns:
        if df[column].isnull().sum() > 0:
            if df[column].dtype in ['float64', 'int64']:
                df[column].fillna(df[column].median(), inplace=True)
            else:
                df[column].fillna(df[column].mode()[0], inplace=True)
    return df
df = handle_missing_values(df)
display(df)

"""**Reduce Noise**"""

def reduce_noise(df):
    """Handles inconsistencies in categorical values by standardizing categories."""
    df['AirBags'] = df['AirBags'].replace({'None': 'No Airbags', 'Driver only': 'Driver', 'Driver & Passenger': 'Both'})
    return df
df = reduce_noise(df)
display(df)

"""**Encode Categorical Features**"""

def encode_categorical(df):
    """Encodes nominal and ordinal categorical features."""
    label_encoders = {}

    # Ordinal Encoding for Airbags
    airbags_order = {'No Airbags': 0, 'Driver': 1, 'Both': 2}
    df['AirBags'] = df['AirBags'].map(airbags_order)

    # One-Hot Encoding for other categorical variables
    df = pd.get_dummies(df, columns=['Type', 'DriveTrain', 'Man.trans.avail', 'Origin'], drop_first=True)
    return df
df = encode_categorical(df)
display(df)

"""**Normalize Features**"""

def normalize_features(df, method='minmax'):
    """Scales numerical features using MinMaxScaler or StandardScaler."""
    scaler = MinMaxScaler() if method == 'minmax' else StandardScaler()
    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns
    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])
    return df
df = normalize_features(df)
display(df)

"""**Split Data**"""

def split_data(df, target_column='Price'):
    """Splits data into train (70%), validation (20%), and test (10%)."""
    X = df.drop(columns=[target_column])
    y = df[target_column]

    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=42)

    return X_train, X_val, X_test, y_train, y_val, y_test

"""# **Q2a: Linear Regression Task.**
Use the “linear_regression_dataset.csv”
Implement the linear regression model to predict the dependency between two variables.
1. Implement linear regression using the inbuilt function “LinearRegression” model in sklearn.
2. Print the coefficient obtained from linear regression and plot a straight line on the scatter plot.
3. Now, implement linear regression without the use of any inbuilt function.
4. Compare the results of 1 and 3 graphically.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Load dataset
def load_linear_data():
    return pd.read_csv(Path("/content/sample_data/linear_regression_dataset.csv"))

linear_df = load_linear_data()

# Selecting two variables: 'Height' as independent, 'Weight' as dependent
X = linear_df[['Height']].dropna()
y = linear_df['Weight'].loc[X.index]

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1. Implement Linear Regression using sklearn
model = LinearRegression()
model.fit(X_train, y_train)

y_pred_sklearn = model.predict(X_test)

# Print coefficient and intercept
print(f"Sklearn Linear Regression: Coefficient = {model.coef_[0]}, Intercept = {model.intercept_}")

# 3. Implement Linear Regression without inbuilt functions
display(X_train)
X_train_np = X_train.to_numpy().flatten()
y_train_np = y_train.to_numpy()
display(X_train_np)
# Calculate slope (m) and intercept (b)
n = len(X_train_np)
mean_x = np.mean(X_train_np)
mean_y = np.mean(y_train_np)
m = np.sum((X_train_np - mean_x) * (y_train_np - mean_y)) / np.sum((X_train_np - mean_x)**2)
b = mean_y - m * mean_x

y_pred_manual = m * X_test.to_numpy().flatten() + b

print(f"Manual Linear Regression: Coefficient = {m}, Intercept = {b}")

# 4. Compare results graphically
display(X_test, y_pred_sklearn)
display(X_test, y_pred_manual)
plt.scatter(X_test, y_test, color='blue', label='Actual Data')
plt.plot(X_test, y_pred_sklearn, color='red', label='Sklearn Prediction')
plt.plot(X_test, y_pred_manual, color='green', linestyle='dashed', label='Manual Prediction')
plt.xlabel('Height')
plt.ylabel('Weight')
plt.legend()
plt.title('Comparison of Linear Regression Models')
plt.show()



"""# **Q2b: Logistic Regression Task.**
Use the “logistic_regression_dataset.csv”
1. Split the dataset into training set and test set in the ratio of 70:30 or 80:20
2. Train the logistic regression classifier (using inbuilt function: LogisticRegression from sklearn).
3. Print the confusion matrix and accuracy.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score

# Load dataset
def load_logistic_data():
    return pd.read_csv(Path("/content/sample_data/logistic_regression_dataset.csv"))

logistic_df = load_logistic_data()

def encode_data(df1):
    """Encodes nominal and ordinal categorical features."""
    # Ordinal Encoding for Gender
    gender_order = {'Male': 0, 'Female': 1}
    df1['Gender'] = df1['Gender'].map(gender_order)

    return df1
logistic_df = encode_data(logistic_df)

# Selecting features and target variable (assuming the last column is the target)
X = logistic_df.iloc[:, :-1]
y = logistic_df.iloc[:, -1]

# Split data into training and test sets (80:20 ratio)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Compute confusion matrix and accuracy
conf_matrix = confusion_matrix(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# Print results
display("Confusion Matrix:")
display(conf_matrix)
display(f"Accuracy: {accuracy:.2f}")

"""# **Q3: SVM**
Use the dataset “Bank_Personal_Loan_Modelling.csv”
1. Store the dataset in your google drive and in Colab file load the dataset from your drive.
2. Check the shape and head of the dataset.
3. Age, Experience, Income, CCAvg, Mortgage, Securities are the features and Credit Card is your Target Variable.
i. Take any 3 features from the six features given above
ii. Store features and targets into a separate variable
iii. Look for missing values in the data, if any, and address them accordingly.
iv. Plot a 3D scatter plot using Matplotlib.
4. Split the dataset into 80:20. (3 features and 1 target variable).
5. Train the model using scikit learn SVM API (LinearSVC) by setting the regularization parameter C as C = {0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000}.
i. For each value of C Print the score on test data
ii. Make the prediction on test data
iii. Print confusion matrix and classification report
6. Use gridSearchCV a cross-validation technique to find the best regularization parameters (i.e.: the best value of C).
In the report provide your findings for the output generated for all the kernels used and also describe the changes that happened after changing the regularization hyperparameter.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

csv_url = "/content/sample_data/Bank_Personal_Loan_Modelling.csv"
# Load dataset
def load_bank_data():
    return pd.read_csv(csv_url)

df = load_bank_data()

# Display basic information about the dataset
print("Dataset Shape:", df.shape)
print("Dataset Head:")
print(df.head())

# Selecting features and ssstarget variable
features = ["Age", "Experience", "Income", "CCAvg", "Mortgage", "Securities Account"]
target = "CreditCard"  # 'Credit Card' is the target variable

# Choosing three features
selected_features = ["Income", "Experience", "Age"]
X = df[selected_features]
y = df[target]

# Handling missing values
X.fillna(X.mean(), inplace=True)
# Split the dataset (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot
ax.scatter(X['Age'], X['Income'], X['Experience'], c=y, cmap='viridis', marker='o')

ax.set_xlabel('Age')
ax.set_ylabel('Income')
ax.set_zlabel('Experience')
ax.set_title('3D Scatter Plot of Features')

plt.show()

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the SVM model with different C values
C_values = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]

for C in C_values:
    model = SVC(kernel='linear', C=C)
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)
    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f"C={C}, Accuracy: {accuracy:.4f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    # Compute confusion matrix and accuracy
    conf_matrix = confusion_matrix(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()

    # Print results
    display("Confusion Matrix:")
    display(conf_matrix)
    display(f"Accuracy: {accuracy:.2f}")
    print("Confusion Matrix:")
    #print(confusion_matrix(y_test, y_pred))
    print("-" * 50)

# Hyperparameter tuning using GridSearchCV
C_values = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]
param_grid = {'C': C_values}
grid_search = GridSearchCV(SVC(kernel='linear'), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print("Best hyperparameter (C):", grid_search.best_params_)

"""# **Q4: Decision Tree and Random Forest**
Load the IRIS dataset. The dataset consists of 150 samples of iris flowers, each belonging to one of three species (setosa, versicolor, or virginica). Each sample includes four features: sepal length, sepal width, petal length, and petal width.
1. Visualize the distribution of each feature and the class distribution.
2. Encode the categorical target variable (species) into numerical values.
3. Split the dataset into training and testing sets (use an appropriate ratio).
4. Decision Tree Model
i. Build a decision tree classifier using the training set.
ii. Visualize the resulting decision tree.
iii. Make predictions on the testing set and evaluate the model's performance using appropriate metrics (e.g., accuracy, confusion matrix).
5. Random Forest Model
i. Build a random forest classifier using the training set.
ii. Tune the hyperparameters (e.g., number of trees, maximum depth) if necessary.
iii. Make predictions on the testing set and evaluate the model's performance using appropriate metrics and compare it with the decision tree model.




"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.datasets import load_iris

# Load the IRIS dataset
iris = load_iris()
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['species'] = iris.target

# Visualize feature distribution
sns.pairplot(df, hue='species')
plt.show()

# Split dataset into training and testing sets
X = df.iloc[:, :-1]
y = df['species']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Decision Tree Model
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)

# Visualizing the Decision Tree
plt.figure(figsize=(12, 8))
plot_tree(dt_model, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()

# Evaluate Decision Tree
y_pred_dt = dt_model.predict(X_test)
print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Decision Tree Confusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))
conf_matrix = confusion_matrix(y_test, y_pred_dt)
accuracy = accuracy_score(y_test, y_pred_dt)
plt.figure(figsize=(6,5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# Random Forest Model
rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
rf_model.fit(X_train, y_train)

# Evaluate Random Forest
y_pred_rf = rf_model.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Random Forest Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
conf_matrix = confusion_matrix(y_test, y_pred_rf)
accuracy = accuracy_score(y_test, y_pred_rf)
plt.figure(figsize=(6,5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()
